"""
Production-ready time series forecasting module for server metrics
"""

import pandas as pd
import numpy as np
from prophet import Prophet
from prophet.diagnostics import cross_validation, performance_metrics
import matplotlib.pyplot as plt
import logging
from typing import Tuple, Optional, Dict, Any, List
import pickle
import json
from datetime import datetime, timedelta
import warnings
from pathlib import Path
import sys
import os

# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π
BASE_DIR = Path(__file__).parent.absolute()
LOG_DIR = BASE_DIR / "logs"
MODEL_DIR = BASE_DIR / "models"
FORECAST_DIR = BASE_DIR / "forecasts"

# –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
for directory in [LOG_DIR, MODEL_DIR, FORECAST_DIR]:
    directory.mkdir(exist_ok=True, parents=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/prophet_forecast.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å warnings
warnings.filterwarnings('ignore')


class Config:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    DEFAULT_HYPERPARAMS = {
        'changepoint_prior_scale': 0.05,
        'seasonality_prior_scale': 10.0,
        'holidays_prior_scale': 10.0,
        'seasonality_mode': 'multiplicative',
        'daily_seasonality': True,
        'weekly_seasonality': True,
        'yearly_seasonality': False,
        'mcmc_samples': 300
    }

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
    FORECAST_PERIODS = 48  # 24 —á–∞—Å–∞ –ø—Ä–∏ 30-–º–∏–Ω—É—Ç–Ω–æ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ
    FORECAST_FREQ = '30min'

    # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
    ANOMALY_THRESHOLD = 3.0  # –í —Å–∏–≥–º–∞—Ö
    CONFIDENCE_LEVEL = 0.95  # –£—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è


class DataPreprocessor:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""

    @staticmethod
    def prepare_data(df: pd.DataFrame, server: str, metric: str) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏"""
        logger.info(f"Preparing data for server={server}, metric={metric}")

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        df_filtered = df.copy()
        df_filtered = df_filtered.sort_values('timestamp')

        # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è Prophet
        prophet_df = pd.DataFrame({
            'ds': pd.to_datetime(df_filtered['timestamp']),
            'y': df_filtered['value'].astype(float)
        })

        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        prophet_df = prophet_df.drop_duplicates(subset=['ds']).sort_values('ds')

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        DataPreprocessor._validate_data(prophet_df)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        prophet_df = DataPreprocessor._handle_missing_values(prophet_df)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
        prophet_df = DataPreprocessor._handle_outliers(prophet_df)

        logger.info(f"Prepared {len(prophet_df)} records, "
                    f"span: {prophet_df['ds'].min()} to {prophet_df['ds'].max()}")

        return prophet_df

    @staticmethod
    def _validate_data(df: pd.DataFrame):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if len(df) < 48:
            raise ValueError(f"Insufficient data: only {len(df)} records. Minimum 48 required.")

        if df['y'].isnull().all():
            raise ValueError("All metric values are null")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
        time_diff = df['ds'].diff().mode()[0]
        logger.info(f"Main time interval: {time_diff}")

    @staticmethod
    def _handle_missing_values(df: pd.DataFrame, method: str = 'linear') -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        if df['y'].isnull().sum() > 0:
            logger.warning(f"Found {df['y'].isnull().sum()} missing values, filling with {method}")

            if method == 'ffill':
                df['y'] = df['y'].ffill().bfill()
            elif method == 'linear':
                df['y'] = df['y'].interpolate(method='linear').bfill().ffill()
            elif method == 'spline':
                df['y'] = df['y'].interpolate(method='spline', order=3).bfill().ffill()

        return df

    @staticmethod
    def _handle_outliers(df: pd.DataFrame, n_sigma: float = 3.0) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –º–µ—Ç–æ–¥–æ–º —Å–∏–≥–º"""
        mean = df['y'].mean()
        std = df['y'].std()

        lower_bound = mean - n_sigma * std
        upper_bound = mean + n_sigma * std

        outliers_mask = (df['y'] < lower_bound) | (df['y'] > upper_bound)

        if outliers_mask.any():
            logger.warning(f"Found {outliers_mask.sum()} outliers, clipping to bounds")
            df.loc[outliers_mask, 'y'] = np.clip(
                df.loc[outliers_mask, 'y'],
                lower_bound,
                upper_bound
            )

        return df


class ProphetOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Prophet"""

    @staticmethod
    def tune_hyperparameters(df: pd.DataFrame,
                             param_grid: Dict[str, list] = None) -> Dict[str, Any]:
        """–ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏"""

        if param_grid is None:
            param_grid = {
                'changepoint_prior_scale': [0.001, 0.01, 0.05, 0.1, 0.5],
                'changepoint_range': [0.8],
                'seasonality_prior_scale': [2.0, 12.0, 24.0, 48.0],
                'seasonality_mode': ['multiplicative']
            }

        logger.info("Starting hyperparameter tuning...")

        best_params = None
        best_mape = float('inf')

        # –ü—Ä–æ—Å—Ç–æ–π grid search (–¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Bayesian Optimization)
        for changepoint in param_grid['changepoint_prior_scale']:
            for seasonality in param_grid['seasonality_prior_scale']:
                for mode in param_grid['seasonality_mode']:
                    try:
                        model = Prophet(
                            daily_seasonality=True,
                            weekly_seasonality=True,
                            yearly_seasonality=False,
                            seasonality_mode=mode,
                            changepoint_prior_scale=changepoint,
                            seasonality_prior_scale=seasonality,
                            mcmc_samples=0
                        )

                        model.fit(df)

                        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
                        df_cv = cross_validation(
                            model,
                            initial='3 days',
                            period='1 day',
                            horizon='1 day',
                            parallel="processes"
                        )

                        df_p = performance_metrics(df_cv)
                        current_mape = df_p['mape'].mean()

                        if current_mape < best_mape:
                            best_mape = current_mape
                            best_params = {
                                'changepoint_prior_scale': changepoint,
                                'seasonality_prior_scale': seasonality,
                                'seasonality_mode': mode,
                                'mape': current_mape
                            }

                    except Exception as e:
                        logger.warning(f"Failed with params {changepoint}, {seasonality}, {mode}: {e}")

        logger.info(f"Best parameters: {best_params}")
        return best_params

    @staticmethod
    def add_custom_seasonalities(model: Prophet, df: pd.DataFrame) -> Tuple[Prophet, pd.DataFrame]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–µ–π"""

        # –†–∞–±–æ—á–∏–µ —á–∞—Å—ã
        df['is_work_hours'] = df['ds'].dt.hour.between(9, 18).astype(float)
        df['is_night_hours'] = df['ds'].dt.hour.between(0, 6).astype(float)
        df['is_work_day'] = df['ds'].dt.weekday.between(0, 4).astype(float)

        model.add_seasonality(
            name='work_hours',
            period=1,
            fourier_order=5,
            condition_name='is_work_hours'
        )

        model.add_seasonality(
            name='night_hours',
            period=1,
            fourier_order=3,
            condition_name='is_night_hours'
        )

        return model, df


class ProductionProphetForecaster:
    """–ü—Ä–æ–¥–∞–∫—à–µ–Ω-–≤–µ—Ä—Å–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤—â–∏–∫–∞"""

    def __init__(self, config: Config = None):
        self.config = config or Config()
        self.model = None
        self.metrics_history = []
        logger.info("ProductionProphetForecaster initialized")

    def train(self, df: pd.DataFrame,
              optimize: bool = False,
              save_model: bool = True) -> 'ProductionProphetForecaster':
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""

        logger.info(f"Training model on {len(df)} records")

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        hyperparams = self.config.DEFAULT_HYPERPARAMS.copy()

        if optimize and len(df) > 7 * 48:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            try:
                best_params = ProphetOptimizer.tune_hyperparameters(df)
                if best_params:
                    hyperparams.update({
                        k: v for k, v in best_params.items()
                        if k in hyperparams
                    })
            except Exception as e:
                logger.error(f"Hyperparameter optimization failed: {e}")

        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = Prophet(**hyperparams)

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–µ–π
        self.model, df = ProphetOptimizer.add_custom_seasonalities(self.model, df)

        # –û–±—É—á–µ–Ω–∏–µ
        self.model.fit(df)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if save_model:
            self.save_model(f"prophet_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl")

        logger.info("Model training completed")
        return self

    def predict(self,
                future_periods: int = None,
                freq: str = None,
                include_history: bool = True) -> Dict[str, Any]:
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""

        if self.model is None:
            raise ValueError("Model not trained. Call train() first.")

        periods = future_periods or self.config.FORECAST_PERIODS
        freq = freq or self.config.FORECAST_FREQ

        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ—Ä–µ–π–º–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
            future = self.model.make_future_dataframe(
                periods=periods,
                freq=freq,
                include_history=include_history
            )

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É—Å–ª–æ–≤–∏–π –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–µ–π
            future['is_work_hours'] = future['ds'].dt.hour.between(9, 18).astype(float)
            future['is_night_hours'] = future['ds'].dt.hour.between(0, 6).astype(float)

            # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
            forecast = self.model.predict(future)

            # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞
            forecast = self._postprocess_forecast(forecast)

            # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
            anomalies = self._detect_anomalies(forecast)

            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = {
                'forecast': forecast,
                'anomalies': anomalies,
                'timestamp': datetime.now().isoformat(),
                'periods': periods,
                'freq': freq
            }

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞
            self._save_forecast(result)

            logger.info(f"Forecast generated for {periods} periods")
            return result

        except Exception as e:
            logger.error(f"Prediction failed: {e}")
            raise

    def _postprocess_forecast(self, forecast: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞"""

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        forecast['yhat'] = forecast['yhat'].clip(lower=0)
        forecast['yhat_lower'] = forecast['yhat_lower'].clip(lower=0)
        forecast['yhat_upper'] = forecast['yhat_upper'].clip(lower=0)

        # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ
        numeric_cols = forecast.select_dtypes(include=[np.number]).columns
        forecast[numeric_cols] = forecast[numeric_cols].round(2)

        return forecast

    def _detect_anomalies(self, forecast: pd.DataFrame) -> pd.DataFrame:
        """–î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –ø—Ä–æ–≥–Ω–æ–∑–µ"""

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è baseline
        historical = forecast[forecast['ds'] < datetime.now()].tail(100)

        if len(historical) > 0:
            mean = historical['yhat'].mean()
            std = historical['yhat'].std()

            future = forecast[forecast['ds'] >= datetime.now()].copy()
            future['z_score'] = (future['yhat'] - mean) / std

            anomalies = future[
                abs(future['z_score']) > self.config.ANOMALY_THRESHOLD
                ].copy()

            if len(anomalies) > 0:
                logger.warning(f"Detected {len(anomalies)} potential anomalies in forecast")

            return anomalies

        return pd.DataFrame()

    def evaluate(self, df: pd.DataFrame) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""

        if len(df) < 48:
            logger.warning("Insufficient data for evaluation")
            return {}

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test.csv
        split_idx = int(len(df) * 0.8)
        train_df = df.iloc[:split_idx]
        test_df = df.iloc[split_idx:]

        # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ train
        self.train(train_df, optimize=False, save_model=False)

        # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ test.csv
        forecast = self.predict(
            future_periods=len(test_df),
            freq=test_df['ds'].diff().mode()[0] or '30min',
            include_history=False
        )['forecast']

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        merged = pd.merge(
            test_df,
            forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']],
            on='ds',
            how='inner'
        )

        if len(merged) > 0:
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            metrics = {
                'mae': np.mean(np.abs(merged['yhat'] - merged['y'])),
                'mse': np.mean((merged['yhat'] - merged['y']) ** 2),
                'rmse': np.sqrt(np.mean((merged['yhat'] - merged['y']) ** 2)),
                'mape': np.mean(np.abs((merged['yhat'] - merged['y']) / merged['y'].clip(lower=0.1))) * 100,
                'smape': 2.0 * np.mean(np.abs(merged['yhat'] - merged['y']) /
                                       (np.abs(merged['yhat']) + np.abs(merged['y']))) * 100,
                'coverage': ((merged['y'] >= merged['yhat_lower']) &
                             (merged['y'] <= merged['yhat_upper'])).mean() * 100
            }

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            self.metrics_history.append({
                'timestamp': datetime.now().isoformat(),
                **metrics
            })

            logger.info(f"Model evaluation metrics: {metrics}")
            return metrics

        return {}

    def save_model(self, filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""

        if self.model is None:
            raise ValueError("No model to save")

        model_path = MODEL_DIR / filename

        with open(model_path, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'config': self.config,
                'timestamp': datetime.now().isoformat()
            }, f)

        logger.info(f"Model saved to {model_path}")

    @classmethod
    def load_model(cls, filename: str) -> 'ProductionProphetForecaster':
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""

        model_path = Path(filename)

        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found: {filename}")

        with open(model_path, 'rb') as f:
            data = pickle.load(f)

        forecaster = cls(data['config'])
        forecaster.model = data['model']

        logger.info(f"Model loaded from {filename}")
        return forecaster

    def _save_forecast(self, result: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤ —Ñ–∞–π–ª"""

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV
        csv_path = FORECAST_DIR / f"forecast_{timestamp}.csv"
        result['forecast'].to_csv(csv_path, index=False)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ JSON
        metadata = {
            'timestamp': result['timestamp'],
            'periods': result['periods'],
            'freq': str(result['freq']) if hasattr(result['freq'], '__str__') else result['freq'],
            'anomalies_count': len(result['anomalies'])
        }

        json_path = FORECAST_DIR / f"forecast_metadata_{timestamp}.json"
        with open(json_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        logger.info(f"Forecast saved to {csv_path}")


class MonitoringDashboard:
    """–î–∞—à–±–æ—Ä–¥ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤"""

    @staticmethod
    def create_dashboard(forecast_result: Dict[str, Any],
                         metrics: Optional[Dict[str, float]] = None):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π"""

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Server Metrics Forecast Dashboard', fontsize=16)

        forecast = forecast_result['forecast']

        # 1. –û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫ –ø—Ä–æ–≥–Ω–æ–∑–∞
        ax1 = axes[0, 0]
        ax1.plot(forecast['ds'], forecast['yhat'], 'b-', label='Forecast', linewidth=2)
        ax1.fill_between(forecast['ds'],
                         forecast['yhat_lower'],
                         forecast['yhat_upper'],
                         alpha=0.2, color='blue', label='Confidence Interval')

        if 'anomalies' in forecast_result and len(forecast_result['anomalies']) > 0:
            ax1.scatter(forecast_result['anomalies']['ds'],
                        forecast_result['anomalies']['yhat'],
                        color='red', s=100, label='Anomalies', zorder=5)

        ax1.set_xlabel('Date')
        ax1.set_ylabel('Metric Value')
        ax1.set_title('Forecast with Confidence Interval')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∞
        ax2 = axes[0, 1]
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        ax2.set_title('Forecast Components')
        ax2.text(0.5, 0.5, 'Component analysis\n(requires Prophet model plot_components)',
                 horizontalalignment='center', verticalalignment='center')
        ax2.axis('off')

        # 3. –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        ax3 = axes[1, 0]
        if metrics:
            metric_names = list(metrics.keys())
            metric_values = list(metrics.values())

            bars = ax3.bar(metric_names, metric_values)
            ax3.set_ylabel('Value')
            ax3.set_title('Model Evaluation Metrics')
            ax3.set_xticklabels(metric_names, rotation=45)

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
            for bar, value in zip(bars, metric_values):
                ax3.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
                         f'{value:.2f}', ha='center', va='bottom')
        else:
            ax3.text(0.5, 0.5, 'No evaluation metrics available',
                     horizontalalignment='center', verticalalignment='center')
            ax3.axis('off')

        # 4. –ê–Ω–æ–º–∞–ª–∏–∏
        ax4 = axes[1, 1]
        if 'anomalies' in forecast_result and len(forecast_result['anomalies']) > 0:
            anomalies = forecast_result['anomalies']
            ax4.bar(range(len(anomalies)), anomalies['z_score'])
            ax4.set_xlabel('Anomaly Index')
            ax4.set_ylabel('Z-Score')
            ax4.set_title(f'Detected Anomalies ({len(anomalies)} found)')
            ax4.axhline(y=3, color='r', linestyle='--', label='Threshold (3œÉ)')
            ax4.axhline(y=-3, color='r', linestyle='--')
            ax4.legend()
        else:
            ax4.text(0.5, 0.5, 'No anomalies detected',
                     horizontalalignment='center', verticalalignment='center')
            ax4.axis('off')

        plt.tight_layout()

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—à–±–æ—Ä–¥–∞
        save_path = Path('forecasts') / f"dashboard_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        logger.info(f"Dashboard saved to {save_path}")


# –û—Å–Ω–æ–≤–Ω–æ–π pipeline
def run_production_pipeline(df_path: str,
                            server: str,
                            metric: str,
                            retrain: bool = False):
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ pipeline –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞"""

    logger.info("=" * 60)
    logger.info(f"Starting production pipeline for {server} - {metric}")
    logger.info("=" * 60)

    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info("Step 1: Loading data...")
        df = pd.read_excel(df_path)
        df['timestamp'] = pd.to_datetime(df['timestamp'],
                                         format="%Y-%m-%d %H:%M:%S",
                                         errors='coerce')

        # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        logger.info("Step 2: Preprocessing data...")
        preprocessor = DataPreprocessor()
        processed_df = preprocessor.prepare_data(df, server, metric)

        # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logger.info("Step 3: Training model...")

        model_filename = f"{server}_{metric}_model.pkl"
        model_path = MODEL_DIR / model_filename

        if retrain or not model_path.exists():
            forecaster = ProductionProphetForecaster()
            forecaster.train(processed_df, optimize=True)
            forecaster.save_model(model_filename)
        else:
            logger.info("Loading existing model...")
            forecaster = ProductionProphetForecaster.load_model(model_path)

        # 4. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        logger.info("Step 4: Evaluating model...")
        metrics = forecaster.evaluate(processed_df)

        # 5. –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
        logger.info("Step 5: Generating forecast...")
        forecast_result = forecaster.predict()

        # 6. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        logger.info("Step 6: Creating dashboard...")
        MonitoringDashboard.create_dashboard(forecast_result, metrics)

        # 7. –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info("Step 7: Exporting results...")

        # –≠–∫—Å–ø–æ—Ä—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞
        forecast_df = forecast_result['forecast'].tail(48)
        forecast_df.to_excel(
            f'forecasts/{server}_{metric}_next_24h_{datetime.now().strftime("%Y%m%d")}.xlsx',
            index=False
        )

        # –≠–∫—Å–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫
        if metrics:
            metrics_df = pd.DataFrame([metrics])
            metrics_df.to_excel(
                f'forecasts/{server}_{metric}_metrics_{datetime.now().strftime("%Y%m%d")}.xlsx',
                index=False
            )

        logger.info("Pipeline completed successfully!")

        return {
            'forecaster': forecaster,
            'forecast': forecast_result,
            'metrics': metrics,
            'status': 'success'
        }

    except Exception as e:
        logger.error(f"Pipeline failed: {e}", exc_info=True)
        return {
            'status': 'error',
            'error': str(e)
        }


def evaluate_prophet_model(model, test_df, forecast_df):
    """–ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ Prophet"""

    # –°–æ–≤–º–µ—â–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞ –∏ —Ñ–∞–∫—Ç–∞
    merged = pd.merge(
        test_df,
        forecast_df[['ds', 'yhat', 'yhat_lower', 'yhat_upper']],
        on='ds'
    )

    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    metrics = {
        'MAE': np.mean(np.abs(merged['yhat'] - merged['y'])),
        'RMSE': np.sqrt(np.mean((merged['yhat'] - merged['y']) ** 2)),
        'MAPE': np.mean(np.abs((merged['yhat'] - merged['y']) / merged['y'])) * 100,
        'Coverage': ((merged['y'] >= merged['yhat_lower']) &
                     (merged['y'] <= merged['yhat_upper'])).mean() * 100
    }

    return metrics


if __name__ == '__main__':
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    import argparse

    parser = argparse.ArgumentParser(description='Server metrics forecasting')
    parser.add_argument('--data', type=str,
                        default='/Users/sweetd0ve/dashboard/data/processed/DataLake-DBN1_cpu.usage.average_2025-11-25 17:00:00_2025-11-30 23:30:00.xlsx',
                         help='Path to data file')
    parser.add_argument('--server', type=str, default='DataLake-DBN1',
                        help='Server name')
    parser.add_argument('--metric', type=str, default='cpu.usage.average',
                        help='Metric name')
    parser.add_argument('--retrain', action='store_true',
                        help='Force retrain model')

    args = parser.parse_args()

    # –ó–∞–ø—É—Å–∫ pipeline
    result = run_production_pipeline(
        df_path=args.data,
        server=args.server,
        metric=args.metric,
        retrain=args.retrain
    )

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    if result['status'] == 'success':
        print(f"\n‚úÖ Forecast completed successfully!")
        print(f"üìä Metrics: {result.get('metrics', {})}")
        print(f"üìà Forecast saved in 'forecasts/' directory")
    else:
        print(f"\n‚ùå Forecast failed: {result.get('error', 'Unknown error')}")
        sys.exit(1)